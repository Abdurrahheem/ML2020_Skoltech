{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 2: Part 2 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1. Bayesian methods (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a dataset $D =(X,y) =\\{(x_i,y_i)\\}^m_{i=1}$, $x_i \\in \\mathbb{R}^d$, $y_i\\in\\mathbb{R}$ it is known,that \n",
    "$$y_i = w^T x_i + \\epsilon$$\n",
    "where $\\epsilon \\sim N(0,\\sigma^2)$, $w  \\sim N(0,\\alpha I)$ . Suppose that $X^T X =I$, where $I$ is the identity matrix. Derive MAP estimation for $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution\n",
    "\n",
    "Probabilistic model of the problem can be expressed as follows:\n",
    "\n",
    "$$\n",
    "p(y|x,w,\\beta) = \\mathcal{N}(y|f(x,w),\\sigma^2)\n",
    "$$\n",
    "\n",
    "And posteriory can be expressed as follows:\n",
    "\n",
    "$$p(w|X_m,Y_m,\\alpha,\\beta) \\sim p(Y_m|X_m,w,\\beta) \\times p(w|\\alpha)$$\n",
    "\n",
    "Wich can be converted to  optimization problem:\n",
    "\n",
    "$$\\hat{w} = \\underset{w}{\\operatorname{argmax}} [ \\space  p(Y_m|X_m,w,\\beta) \\times p(w|\\alpha)]$$\n",
    "\n",
    "\n",
    "$$\\hat{w} = \\underset{w}{\\operatorname{argmax}} [ \\log  p(Y_m|X_m,w,\\beta) + \\log p(w|\\alpha)]$$\n",
    "\n",
    "Following manipulatios below we can write optimization problem into a form which is easier to deal with:\n",
    "\n",
    "$$\\log(  pY_m|X_m,w,\\beta) = \\log[\\prod_{i=1}^m \\mathcal{N}(y_i|f(x_i,w),\\underbrace{\\beta}_{\\sigma^2})] = \\log \\prod_{i=1}^m \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp{( \\frac{y_i - f(x_i,w)^2}{2\\sigma^2} )} $$\n",
    "\n",
    "$$\\log(  pY_m|X_m,w,\\beta) = \\underbrace { \\log \\left(\\frac{1}{\\sigma \\sqrt{2\\pi}}\\right)^m }_{C_1 } + \\sum_{i=1}^m \\log \\left( e^{- \\frac{(y_i - f(x_i,w))^2}{2 \\sigma^2}} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log(  pY_m|X_m,w,\\beta) = C_1 - \\frac{1}{2\\sigma^2} \\sum_{i=1}^m( y_i - f(x_i,w))^2\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\log(  pY_m|X_m,w,\\beta) = C_1 - \\frac{1}{2\\sigma^2} || Y - XW||_2^2\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\log p(w|\\alpha) =\\log \\mathcal{N}(w|0,\\alpha I)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log p(w|\\alpha)  = \\underbrace{\\log \\left(\\frac{1}{2\\pi\\alpha } \\right)^d}_{C_2} + \\log \\prod_{i=1}^d e^{\\frac{-w_iw_i^T}{2\\alpha}}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\log p(w|\\alpha) = C_2 + \\sum_{i=1}^d log( e^{\\frac{-w_iw_i^T}{2\\alpha}})\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\log p(w|\\alpha) = C_2 - \\frac{1}{2\\alpha}||W||_2^2\n",
    "$$\n",
    "\n",
    "The optimzation is reduced to the following from where the constants $ C_1, C_2 $ can be discared as they do not effect argmax and the argmax is effectively converted to argmin in order to get rid of minus terms\n",
    "\n",
    "$$\n",
    "\\hat{w} = \\underset{w}{\\operatorname{argmax}} [ C_1 - \\frac{1}{2\\sigma^2} || Y - XW||_2^2 + C_2 - \\frac{1}{2\\alpha}||W||_2^2]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{w} = \\underset{w}{\\operatorname{argmin}} [  \\frac{1}{2\\sigma^2} || Y - XW||_2^2 +  \\frac{1}{2\\alpha}||W||_2^2]\n",
    "$$\n",
    "\n",
    "And the valeus of W that minimize the fuction are found as below:\n",
    "$$\n",
    "\\bigtriangledown_w \\left( \\frac{1}{2\\sigma^2} || Y - XW||_2^2 +  \\frac{1}{2\\alpha}||W||_2^2 \\right) = \\frac{1}{\\sigma^2}X^T(XW-Y)+\\frac{1}{\\alpha}W\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma^2}X^T(XW-Y)+\\frac{1}{\\alpha}W = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "W  = (XX^T + \\frac{\\sigma^2}{\\alpha}I)^{-1}X^TY = \\frac{1}{1+\\frac{\\sigma^2}{\\alpha}}X^TY\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Gaussian Processes 1 (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\sigma_n(\\mathbf{x}_*)$ be a predictive variance at point $\\mathbf{x}_*$ of a Gaussian Process $f_n$ with zero mean and covariance $k(\\cdot,\\cdot)$ that was built using first $n$ training points.\n",
    "Prove that for $\\forall \\mathbf{x}_*$ it holds\n",
    "$$\n",
    "    \\sigma_{n}(\\mathbf{x}_*) \\leq \\sigma_{n-1}(\\mathbf{x}_*).\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution\n",
    "\n",
    "\n",
    "**Before you start checking this question I want to point out the questions in this homework are  not just one step solutions as you have said in the canvas discussions. In fact, most of the question require deep mathemathical proofs** \n",
    "\n",
    "\n",
    "Writing following equation holds true for posterior variance of a Gaussian Process:\n",
    "$$\n",
    "\\sigma_n({\\bf{x}_*}) = k({\\bf{x}_*},{\\bf{x}_*}) - {\\bf{k_*}}^T(K_n + \\sigma^2I_n)^{-1}{\\bf{k_*}}\n",
    "$$\n",
    "\n",
    "What we really want to do is to prove that the second term of the expression above for $\\sigma_{n-1}(\\mathbf{x}_*)$ is smaller than for $\\sigma_{n}(\\mathbf{x}_*)$. From equation above can ignore $ k({\\bf{x}_*},{\\bf{x}_*})$ since it's the same for $\\sigma_{n-1}(\\mathbf{x}_*)$. Let's find ${\\bf{k_*}}^T(K_n + \\sigma^2I_n)^{-1}{\\bf{k_*}}$ using the next lemma:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "A = \\begin{pmatrix} P & Q \\\\ R & S \\end{pmatrix}, \\qquad A^{-1} = \\begin{pmatrix} \\tilde P & \\tilde Q \\\\ \\tilde R & \\tilde S \\end{pmatrix},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde P & = P^{-1} + P^{-1}QMRP^{-1},\\\\\n",
    "\\tilde Q & = -P^{-1}QM,\\\\\n",
    "\\tilde R & = -MRP^{-1},\\\\\n",
    "\\tilde S & = M,\\\\\n",
    "M & =  (S - RP^{-1}Q)^{-1}.\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Gramm matric can be decomposed as below, where $x'$ is $n^{th}$ train point:\n",
    "$$\n",
    "\\begin{align}\n",
    "K_n + \\sigma^2 I_n = \n",
    "\\begin{pmatrix}\n",
    "K_{n-1} + \\sigma^2 I_{n-1} & k_{n-1}(x')\\\\k_{n-1}(x')^\\top & k(x',x') + \\sigma^2\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Using provided lemma above the inverse follows as\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "(K_n + \\sigma^2 I_n)^{-1} = \\begin{pmatrix}\\kappa + \\kappa k_{n-1}(x')Mk_{n-1}(x')^\\top\\kappa & -\\kappa k_{n-1}(x')M \\\\ -Mk_{n-1}(x')^\\top \\kappa & M \\end{pmatrix},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "M & = (k(x', x') + \\sigma^2 - k_{n-1}(x')^\\top(K_{n-1} + \\sigma^2I_{n-1})^{-1}k_{n-1}(x'))^{-1}, \\\\\n",
    "\\kappa & = (K_{n-1} + \\sigma^2I_{n-1})^{-1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Simplyfing $(K_n + \\sigma^2 I_n)^{-1}$ by multiplyng the terms:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\bf{k_*}}^T(K_n + \\sigma^2I_n)^{-1}{\\bf{k_*}} = & \\begin{pmatrix}k_{n-1}(x^*)\\\\k'(x^*)\\end{pmatrix}^\\top\\begin{pmatrix}\\kappa + \\kappa k_{n-1}(x')Mk_{n-1}(x')^\\top\\kappa & -\\kappa k_{n-1}(x')M \\\\ -Mk_{n-1}(x')^\\top\\kappa  & M \\end{pmatrix}\\begin{pmatrix}k_{n-1}(x^*)\\\\k'(x^*)\\end{pmatrix}\\\\\n",
    "& = \\begin{pmatrix} k_{n-1}^\\top(x^*)\\kappa + k_{n-1}^\\top(x^*)\\kappa k_{n-1}(x')Mk_{n-1}(x')^\\top\\kappa - k'(x^*)Mk_{n-1}(x')^\\top \\kappa \\\\ -k_{n-1}^\\top(x^*)\\kappa k_{n-1}(x')M + k'(x^*)M \\end{pmatrix} ^\\top\n",
    "\\begin{pmatrix}k_{n-1}(x^*)\\\\k'(x^*)\\end{pmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\bf{k_*}}^T(K_n + \\sigma^2I_n)^{-1}{\\bf{k_*}} & = k_{n-1}^\\top(x^*)\\kappa k_{n-1}(x^*) + k_{n-1}^\\top(x^*)\\kappa k_{n-1}(x')Mk_{n-1}(x')^\\top\\kappa k_{n-1}(x^*)\\\\\n",
    "& - k'(x^*)Mk_{n-1}(x')^\\top\\kappa k_{n-1}(x^*) - k_{n-1}(x^*)^\\top\\kappa k_{n-1}(x')Mk'(x^*) + k'(x^*)Mk'(x^*).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Where $ \\kappa = (K_{n-1} + \\sigma^2 I_{n-1})^{-1}$ as before and $k'(x^*) = k(x',x^*)$. As the fist term of the above expresson relates to  $\\sigma_{n-1}(\\mathbf{x}_*)$ what needs to be done is to show that the following terms are noen-negative for $    \\sigma_{n}(\\mathbf{x}_*) \\leq \\sigma_{n-1}(\\mathbf{x}_*)$ to hold \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\bf{k_*}}^T(K_n + \\sigma^2I_n)^{-1}{\\bf{k_*}} & = k_{n-1}^\\top(x^*)\\kappa k_{n-1}(x^*) + (\\alpha - k'(x^*))^2\\\\\n",
    "& = k_{n-1}(x^*)^\\top(K_{n-1} + \\sigma^2I_{n-1})^{-1}k_{n-1}(x^*) + \\tfrac{1}{M}(\\alpha - k'(x^*))^2\\\\\n",
    "& \\leq k_{n-1}(x^*)^\\top(K_{n-1} + \\sigma^2I_{n-1})^{-1}k_{n-1}(x^*).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\\sigma_{n}(\\mathbf{x}_*) \\leq \\sigma_{n-1}(\\mathbf{x}_*).\n",
    "$$\n",
    "Where $\\alpha = k_{n-1}^T(x^*)\\kappa k_{n-1}(x')$. The above expression holds true when $\\tfrac{1}{M}(k_{n-1}^\\top(x^*)(K_{n-1} + \\sigma^2I_{n-1})^{-1} k_{n-1}(x') - k(x^*, x'))$ is non-negative\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The solution to his problem could be found [here](https://stats.stackexchange.com/questions/409980/gaussian-process-why-adding-data-points-cannot-increase-the-predictive-bias/41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3. Gaussian Processes 2 (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider you have gaussian distribution on $R$ with zero mean and differentiable by arguments covariation funtion $k(x, \\tilde{x})$.Get an expression for the correlation between the implementation of a Gaussian process  $y(x) ∼ GP (0, k(x, x ^{\\prime}))$ and its derivative $\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution\n",
    "\n",
    "Let us define pearson correlation as follows:\n",
    "$$\n",
    "p(x,y) = \\frac{cov(x,y)}{\\sigma_x \\sigma_y}\n",
    "$$\n",
    "and where: \n",
    "\n",
    "$cov(x,y) = \\mathbb{E}(x,y) - \\mathbb{E}(x)\\mathbb{E}(y)$\n",
    "\n",
    "What we need to do is to express $x$ and $y$ in terms of $y(x)$ and $\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x}$ \n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "cov(y(x),\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x} ) = \\mathbb{E}(y(x),\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x} ) - \\mathbb{E}(y(x))\\mathbb{E}(\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x})\\\\\n",
    "= \\mathbb{E}(y(x),\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x} ) -0\\times\\mathbb{E}(\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x} )\n",
    "\\\\\n",
    "= \\mathbb{E}(y(x),\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x} )\\\\\n",
    "=\\frac{\\partial }{\\partial \\tilde x} \\mathbb{E}(y(x),y(\\tilde x) ) = \\frac{\\partial }{\\partial \\tilde x} \\kappa(x,\\tilde x)\n",
    "$$\n",
    "\n",
    "\n",
    "$\\mathbb{E}(y(x)) = 0$, becasue the mean of the $GP$ is 0. Let's now rewerite the $\\sigma$'s:\n",
    "\n",
    "$$\\sigma_{y(x)}^2 = cov(y(x),y(\\tilde x))= \\kappa(x,x')$$\n",
    "\n",
    "$$\\sigma_{\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x} }^2 = \\mathbb{E}(\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x},\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x}) = \\frac{\\partial^2}{\\partial \\tilde x^2} \\mathbb{E}(y(\\tilde x),y(\\tilde x)) = \\frac{\\partial^2}{\\partial \\tilde x^2}  \\kappa (\\tilde x,\\tilde x)$$\n",
    "\n",
    "Now let's rewrite the pearson correlation for our GP:\n",
    "$$\n",
    "p(y(x), \\frac{\\partial y(\\tilde x)}{\\partial \\tilde x}) = \\frac{ \\frac{\\partial }{\\partial \\tilde x} \\kappa(x,\\tilde x)}{\\sqrt{ \\kappa(x,x')} \\times \\sqrt{\\frac{\\partial^2}{\\partial \\tilde x^2}  \\kappa (\\tilde x,\\tilde x)} }\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4. Kernel theory (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $K(x, x'):\\mathcal{X}\\times \\mathcal{X}\\rightarrow \\mathbb{R}$ be a PDS kernel,\n",
    "and $\\phi\\colon \\mathcal{X} \\to \\mathcal{H}$ its <b>unknown </b> feature mapping. For $x,x'\\in\\mathcal{X}$ derive the formula for the **distance** between $\n",
    "\\phi(x)$ and $\\phi(x')$ in $\\mathcal{H}$.\n",
    "\n",
    "\n",
    "Let us define the following:\n",
    "\n",
    "$$\n",
    "||\\phi(x)  - \\phi(x')||_2^2  = < \\phi(x) - \\phi(x'),\\phi(x) - \\phi(x')>  \n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "< \\phi(x) - \\phi(x'),\\phi(x) - \\phi(x')> = <\\phi(x),\\phi(x)> + <\\phi(x'),\\phi(x')> - 2 <\\phi(x),\\phi(x')>\n",
    "$$\n",
    "\n",
    "$$\n",
    "||\\phi(x)  - \\phi(x')||_2^2 =  K(x,x)+K(x',x')-2K(x,x')\n",
    "$$\n",
    "\n",
    "We know that Eucledian distance is: $  \\sqrt{||\\phi(x)  - \\phi(x')||_2^2 }  = \\sqrt{ K(x,x)+K(x',x')-2K(x,x')} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5. Naive Gradient Boosting Regression (1 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a regression dataset, consisting of 5 samples with 1-dimensional feature vector $X$ and scalar target vector $y \\in \\mathbb{R}$:\n",
    "\n",
    "|  x   |  y   | \n",
    "|:----:|:----:| \n",
    "|  10  |  1   | \n",
    "|  32  |  9   | \n",
    "|  46  |  13  | \n",
    "|  54  |  16  | \n",
    "|  63  |  23  | \n",
    "\n",
    "In this task you are asked to implement **3 steps of Gradient Boosting Regression** with decision tree stumps as the learners $h_0, h_1, h_2$. \n",
    "\n",
    "In order to complete this task:\n",
    "1. Refer to the slides on naive boosting for regression in **Lecture 8**.\n",
    "2. Assume that the initial model $f_0$ is the mean of the feature vector $X$\n",
    "3. According to the algorithm on the boosting approach for regression from **1.**, compute the residuals\n",
    "4. Manually, find a suitable split among the $x_i$ for each decision tree weak model $h_t(X)$, which minimizes the loss function:\n",
    "\n",
    "$$L_{\\text{split_i}} = \\frac{\\text{Var}_{left\\_split}*N_{1} + \\text{Var}_{right\\_split}*N_{2}}{N_{1}+N_{2}}$$\n",
    "\n",
    "where  $\\text{Var}$ is the variance of the values contained in each leaf, $N_1$ is the number of target values $y$ in the left leaf, $N_{2}$ - in the right leaf\n",
    "\n",
    "5. Perform the Gradient Boosting step on the ensemble model $f_t$ with the resulting decision tree stump predictions (assume that the learning rate $lr=1.0$).\n",
    "\n",
    "**Note on Decision Tree Stumps:** A decision tree stump is a decision tree, which consists only of the root and its immediate leaves. In case of this task, at each iteration you are asked to consider 5 different variants of the decision tree stumps $h_t^i$ - one variant for each of the split candidates $x_i$. You should choose the variant that minimizes the loss written above. The two leaves of the tree are formed according to the rule:\n",
    "\n",
    "```python\n",
    "if x_i < split:\n",
    "    target_value -> left leaf\n",
    "elif x_i >= split:\n",
    "    target_value -> right leaf\n",
    "```\n",
    "**HINT:** Think about what should be `target_value` equal to in case of Gradient Boosting Regression.\n",
    "\n",
    "The prediction of decision tree stump $h_t(x_i)$ is the mean of the values of the according leaf.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>f0</th>\n",
       "      <th>y-f0</th>\n",
       "      <th>L0</th>\n",
       "      <th>h0</th>\n",
       "      <th>f1</th>\n",
       "      <th>y-f1</th>\n",
       "      <th>L1</th>\n",
       "      <th>h1</th>\n",
       "      <th>f2</th>\n",
       "      <th>y-f2</th>\n",
       "      <th>L2</th>\n",
       "      <th>h2</th>\n",
       "      <th>F3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>12.4</td>\n",
       "      <td>-11.4</td>\n",
       "      <td>53.44</td>\n",
       "      <td>-7.4</td>\n",
       "      <td>5</td>\n",
       "      <td>-4</td>\n",
       "      <td>16.9333</td>\n",
       "      <td>-1.41667</td>\n",
       "      <td>3.58333</td>\n",
       "      <td>-2.58333</td>\n",
       "      <td>8.90556</td>\n",
       "      <td>-2.58333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>12.4</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>20.95</td>\n",
       "      <td>-7.4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>12.9333</td>\n",
       "      <td>-1.41667</td>\n",
       "      <td>3.58333</td>\n",
       "      <td>5.41667</td>\n",
       "      <td>7.23715</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>4.22917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46</td>\n",
       "      <td>13</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>16.9333</td>\n",
       "      <td>4.93333</td>\n",
       "      <td>17.3333</td>\n",
       "      <td>-4.33333</td>\n",
       "      <td>16.9333</td>\n",
       "      <td>-1.41667</td>\n",
       "      <td>15.9167</td>\n",
       "      <td>-2.91667</td>\n",
       "      <td>7.56759</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>16.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>16</td>\n",
       "      <td>12.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>19.8333</td>\n",
       "      <td>4.93333</td>\n",
       "      <td>17.3333</td>\n",
       "      <td>-1.33333</td>\n",
       "      <td>13.8037</td>\n",
       "      <td>-1.41667</td>\n",
       "      <td>15.9167</td>\n",
       "      <td>0.0833333</td>\n",
       "      <td>8.9044</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>16.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>63</td>\n",
       "      <td>23</td>\n",
       "      <td>12.4</td>\n",
       "      <td>10.6</td>\n",
       "      <td>25.35</td>\n",
       "      <td>4.93333</td>\n",
       "      <td>17.3333</td>\n",
       "      <td>5.66667</td>\n",
       "      <td>8.90556</td>\n",
       "      <td>5.66667</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>8.90556</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>23.6458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x   y    f0  y-f0       L0       h0       f1     y-f1       L1       h1  \\\n",
       "1  10   1  12.4 -11.4    53.44     -7.4        5       -4  16.9333 -1.41667   \n",
       "2  32   9  12.4  -3.4    20.95     -7.4        5        4  12.9333 -1.41667   \n",
       "3  46  13  12.4   0.6  16.9333  4.93333  17.3333 -4.33333  16.9333 -1.41667   \n",
       "4  54  16  12.4   3.6  19.8333  4.93333  17.3333 -1.33333  13.8037 -1.41667   \n",
       "5  63  23  12.4  10.6    25.35  4.93333  17.3333  5.66667  8.90556  5.66667   \n",
       "\n",
       "        f2       y-f2       L2        h2       F3  \n",
       "1  3.58333   -2.58333  8.90556  -2.58333        1  \n",
       "2  3.58333    5.41667  7.23715  0.645833  4.22917  \n",
       "3  15.9167   -2.91667  7.56759  0.645833  16.5625  \n",
       "4  15.9167  0.0833333   8.9044  0.645833  16.5625  \n",
       "5       23          0  8.90556  0.645833  23.6458  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "df = pd.DataFrame(index=[1,2,3,4,5],columns=['x','y','f0','y-f0','L0','h0','f1','y-f1','L1','h1','f2','y-f2','L2','h2','F3'])\n",
    "\n",
    "df.x=[10,32,46,54,63]\n",
    "df.y=[1,9,13,16,23]\n",
    "df.f0=np.mean(df.y)\n",
    "df['y-f0']=df.y-df.f0\n",
    "\n",
    "def loss(df,nL,res):\n",
    "  for i,split in enumerate(df.x):\n",
    "      N1 = len(df.x[df.x < split])\n",
    "      N2 = len(df.x[df.x >= split])\n",
    "      var_l = np.var(df[res][df.x < split])\n",
    "      if N1==0:\n",
    "        var_l = 0\n",
    "      var_r = np.var(df[res][df.x >= split])\n",
    "      if N2 == 0:\n",
    "        var_r = 0\n",
    "      L = (var_l*N1 + var_r*N2)/(N1+N2)\n",
    "      df[nL][i+1] = L\n",
    "  mloss = np.argmin(df.L0)\n",
    "  return df,mloss\n",
    "\n",
    "df,mspit = loss(df,'L0','y-f0')\n",
    "df.h0[:2] = np.mean(df['y-f0'][:2])\n",
    "df.h0[2:] = np.mean(df['y-f0'][2:])\n",
    "df.f1 = df.h0+df.f0\n",
    "df['y-f1'] = df.y - df.f1\n",
    "df,_=loss(df,'L1','y-f1')\n",
    "df.h1[:4] = np.mean(df['y-f1'][:4])\n",
    "df.h1[4:] = np.mean(df['y-f1'][4:])\n",
    "df.f2 = df.f1+df.h1\n",
    "df['y-f2'] = df.y-df.f2\n",
    "df,_ = loss(df,'L2','y-f2')\n",
    "df.h2[:1] = np.mean(df['y-f2'][:1])\n",
    "df.h2[1:] = np.mean(df['y-f2'][1:])\n",
    "df.F3 = df.h2+df.f2\n",
    "df.round(decimals=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The task**: My table is above\n",
    "\n",
    "* Fill in the table - round the values of table up to the second digit after decimal point:\n",
    "\n",
    "\n",
    "|   x  |   y  |$f_0$|$$y - f_0$$|$L$|$h_0$|$f_1$|$$y-f_1$$|$L$|$h_1$|$f_2$|$$y - f_2$$|$L$|$h_2$|$F_3$|\n",
    "|------|------|-----|-----------|---|-----|-----|---------|---|-----|-----|-----------|---|-----|-----|\n",
    "|  10  |  1   |  41 |  12.4      |-11.4 |  53.44  |  -7.4  |5     |-4     |  0  |  0  |    0      | 0 |  0  |  0  | \n",
    "|  32  |  9   |  41 |  12.4      | -3.4 |  20.95  |  -7.4  |5     | 4     |  0  |  0  |    0      | 0 |  0  |  0  |\n",
    "|  46  |  13  |  41 |  12.4      | 0.6  |  16.93  |  4.93  |17.33 | -4.33 |  0  |  0  |    0      | 0 |  0  |  0  |\n",
    "|  54  |  16  |  41 |  12.4      | 3.6  |  19.83  |  4.93  |17.33 | -1.33 |  0  |  0  |    0      | 0 |  0  |  0  |\n",
    "|  63  |  23  |  41 |  12.4      | 10.6 |  25.35  |  4.93  |17.33 | 5.66  |  0  |  0  |    0      | 0 |  0  |  0  |\n",
    "\n",
    "\n",
    "where $L$ is the loss, calculated by the formula for decision tree stumps above, for each of the 5 split variants of the decision tree stump at each iteration\n",
    "* Write down the splits (the feature values) you have found for each of the tree stumps\n",
    "\n",
    "* Insert the predictions of the full ensemble model and the split values, you have achieved after 3 iterations into the plotting cell below (**COPY AND PASTE** the last column from the table above and the splits list to the plotting cell below, instead of **#your solution**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean \n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tree(x,F,stumps):\n",
    "    x_range = np.arange(np.min(x), np.max(x)+1)\n",
    "    x_r = []\n",
    "    f_r = []\n",
    "    stmps = [0] + stumps + [np.inf]\n",
    "    for st in range(1,len(stmps)):\n",
    "        x_r.extend([list(group) for k, group in groupby(x_range, lambda x: x<stmps[st] and x>=stmps[st-1]) if k])\n",
    "        f_r.append([f_i for f_i,x_ii in zip(F,x) if x_ii<stmps[st] and x_ii>=stmps[st-1]])\n",
    "    F_to_plot = []\n",
    "    for ft in range(len(f_r)):\n",
    "        #assert len(f_r) == len(x_r)\n",
    "        if len(f_r[ft]) == 1:\n",
    "            F_to_plot.extend([f_r[ft][0]]*len(x_r[ft]))\n",
    "        elif len(f_r[ft]) > 1:\n",
    "            F_to_plot.extend([mean(f_r[ft])]*len(x_r[ft]))\n",
    "    return F_to_plot,x_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTTING CELL##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [10,32,46,54,63]\n",
    "y = [1, 9, 13, 16, 23]\n",
    "\n",
    "#note that the order of F(x_i) should be corresponding to the order of x_i in the table\n",
    "\n",
    "############ INSERT YOUR SOLUTION HERE###############\n",
    "F3 = [1,4.22,16.56,16.56,23.64]\n",
    "splits = [46,63,32]\n",
    "\n",
    "boosted_F_plot,x_range = plot_tree(x, F3, stumps = list(np.sort(splits)))\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "ax.scatter(x,y, label = 'original')\n",
    "ax.scatter(x, F3, label = 'predicted')\n",
    "ax.plot(x_range,boosted_F_plot,'--', linewidth=2, label = 'composite function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6. AdaBoost (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following cases,explain how AdaBoost, as given in **Lecture 7**, will treat a weak hypothesis $h_t$ with weighted error $N_t(h_t , w_t )$. Also, in each case, explain why this behavior takes place.\n",
    "1. $N_t = \\frac{1}{2}$\n",
    "2. $N_t > \\frac{1}{2}$\n",
    "3. $N_t = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution:\n",
    "1. $ N_t = \\frac{1}{2}, \\space \\alpha_t = 0 $, thus we discart $h_t$ from $\\sum_i^T \\alpha_t * h_t$ the result does not change form $h_t$\n",
    "\n",
    "2. $ N_t > \\frac{1}{2}, \\alpha_t \\space $ is always equal to some negative number and thus it's predicticitons are always inversed. The more nagative $\\alpha_t$ is the more correct predictions $h_t$ will make through inversion.\n",
    "\n",
    "3. $ N_t = 0 $, means that the $h_t$ has made no mistakes, thus is a perfect classifier. The machine for $ N_t = 0, \\space \\frac{1-N_t}{N_t}  = \\epsilon, $ where $\\epsilon$ is a tiny number thus $\\alpha_t $ is a very big number which means that $ h_t $ has a massive weight in the final prediction. Also, weights $W_{i,t+1}$ will not change since $h_t$ has made no mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
