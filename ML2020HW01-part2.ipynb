{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 1: Part 2 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Linear Regression (1 point)\n",
    "Let us consider the problem of linear regression for 2D data $(x_{1},y_{1}),\\dots,(x_{n},y_{n})\\in\\mathbb{R}^{2+ 1}$. Let us have $l_{\\infty}$ regularization penalty, i.e. the optimization problem is\n",
    "$$\n",
    "||Xw - y||_2^2 + \\lambda||w||_{\\infty} \\rightarrow \\min_{\\boldsymbol{w}}\n",
    "$$\n",
    "\n",
    "Show that this problem is equal to Lasso regression problem with feature matrix $Z = XA \\in \\mathbb{R}^{n \\times 2}$ for a certain $2 \\times 2$ matrix $A$ and the same target $y$.\n",
    "### Your solution:\n",
    "\n",
    "Image that for some matrix $A$ we can introduce new set of *weights* - $\\hat{w}$ such that:\n",
    "\n",
    "$$\\hat{w} =Aw$$\n",
    "\n",
    "This way we now can represent the old optimization problem with regularization $l_{1}$, and considering that $Z = XA$, as follows:\n",
    "\n",
    "$$\n",
    "||XAw - y||_2^2 + \\lambda||Aw||_{1} \\rightarrow \\min_{\\boldsymbol{Aw}}\n",
    "$$\n",
    "\n",
    "$$\\Downarrow$$\n",
    "\n",
    "$$ \n",
    "||X \\hat{w} - y||_2^2 + \\lambda|| \\hat{w}||_{1} \\rightarrow \\min_{\\boldsymbol{\\hat{w}}}\n",
    "$$\n",
    "\n",
    "And the claim is that and the optimazation problems are equal under the following conditions:\n",
    "\n",
    "$$  \\lambda||w||_{\\infty} = \\lambda|| Aw||_{1}, \\quad \\textrm{where} \\quad\n",
    "A = \\sqrt{2}\n",
    "\\begin{bmatrix}\n",
    "    cos\\theta & -sin\\theta \\\\\n",
    "    sin\\theta & cos\\theta  \\\\\n",
    "\\end{bmatrix},\n",
    "\\theta=\\frac{\\pi}{4}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Probit Regression (1 point)\n",
    "Let us consider the Probit regression model for a binary classification problem. It is a linear model analogous to the Logistic Regression. For a feature vector $x \\in \\mathbb{R}^d$ the probability for label $y$ being equal to 1 is given by\n",
    "$$P(y=1|x) = \\Phi(x^Tw + b).$$ \n",
    "Here $\\Phi(x)=\\int_{-\\infty}^{x}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^{2}}{2}}dt$ is the Cumulative Density Function for the **standard normal distribution**; values $w \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$ are learnable parameters of the probit regression model.\n",
    "\n",
    "Write down the optimization problem for training probit  probit regression **without L2-regularization** and show that it does not have optimum in the case when the training set is **lineary separable**.\n",
    "\n",
    "### Your solution:\n",
    "\n",
    "Optimazation can be written as follows:\n",
    "$$ arg \\min_{w}\\sum_{i=1}^n(y_i log(\\Phi(x_i^Tw + b)) +(1-y_i)log(1-\\Phi(x_i^Tw + b))) $$\n",
    "\n",
    "In the case of **linearly separable** classes, the funtion $\\Phi(x^Tw + b)$ must resemple a step fuction in order to seperate the two classes. Thus, $x \\to \\infty $ in $\\int_{-\\infty}^{x}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^{2}}{2}}dt$ for $\\Phi(x^Tw + b)$ to resemble a step fuction. As $b$ is only responsible for a shift of $\\Phi(x^Tw + b)$ function it's values will be fixed. Thus we only can change the wights $w$  in $ \\Phi(x^Tw + b)$. As one can see **wihtout regularization** the $ \\Phi(x^Tw + b)$ does not have an optimum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Multiclass Bayesian Naive Classifier (1 point)\n",
    "Let us consider multiclass classification problem with classes $C_1, \\ldots, C_K$. Assume that all $d$ features $x_1, \\dots,x_d$ are binary, i.e. $x_{i}\\in\\{0,1\\}$ for $i=1,2\\dots,d$. Show that the decision rule of a Bayesian Naive Classifier can be represented as $\\arg \\max$ of linear functions of the input. \n",
    "### Your solution:\n",
    "As $x_i$ is **binary** feature $\\in\\{0,1\\}$  then Bernoulli NB is as follows:\n",
    "\n",
    "$$p(x_j,j\\in d|y=k, k \\in K)=\\prod_{j\\in d}p_{kj}^{x_j}(1-p_{kj})^{(1-x_j)} $$\n",
    "\n",
    "Give that, maximum a posteriori(MAP) decision rule cab be written as follows:\n",
    "$$\\hat{y}=arg \\max_{y\\in\\{1..K\\}}p(y|x)=p(y=k)\\cdot\\prod_{j\\in d}p_{kj}^{x_j}(1-p_{kj})^{(1-x_j)} $$\n",
    "\n",
    "The expression above can be coverted into a logorithmic form as such:\n",
    "\n",
    "$$\\hat{y}=arg  \\max_{y\\in\\{1..K\\}} log(p(y|x)) = log(p(y=k)) + \\sum_{j \\in d}x_jlog(p_{kj}) + \n",
    "\\sum_{j \\in d}(1-x_j)log(1-p_{kj})$$\n",
    "\n",
    "$$\\hat{y}=arg  \\max_{y\\in\\{1..K\\}} log(p(y|x)) = log(p(y=k)) + \\sum_{j \\in d}log(1-p_{kj}) + \\sum_{j \\in d}x_j log(p_{kj}) - \n",
    "\\sum_{j \\in d} x_j log(1-p_{kj})$$\n",
    "\n",
    "$$\\hat{y}=arg  \\max_{y\\in\\{1..K\\}} log(p(y|x)) = log(p(y=k)) + \\sum_{j \\in d}log(1-p_{kj}) + \\sum_{j \\in d}x_j(log(p_{kj} - \n",
    "log(1-p_{kj})) $$\n",
    "\n",
    "$$\\hat{y}=arg  \\max_{y\\in\\{1..K\\}} log(p(y|x)) = log(p(y=k)) + \\sum_{j \\in d}log(1-p_{kj}) + \\sum_{j \\in d}x_j log(\\frac{ p_{kj}}{ 1-p_{kj}})$$\n",
    "\n",
    "\n",
    "Thus, there is a linear classifer in a log-scale:\n",
    "$$\\hat{y} = arg \\max_{y\\in\\{1..K\\}} b + w_kx^T,  $$ \n",
    "\n",
    "where:\n",
    "$$b = log(p(y=k)) + \\sum_{j \\in d}log(1-p_{kj}), j \\in d $$\n",
    "$$w_k = log(\\frac{ p_{kj}}{ 1-p_{kj}})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Nearest Neighbors (1 point)\n",
    "Consider the 1-nearest-neighbor classifier applied to multiclass classification problem. Let's denote the classifier fitted on data $X$ by $f_X(\\cdot)$.\n",
    "\n",
    "The formula for complete **leave-k-out cross-validation** on data sample $X^{n}$ is defined as\n",
    "$$L_{k}OCV=\\frac{1}{C_{n}^{k}}\\cdot\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k}\\bigg(\\sum_{i\\in X}[y_{i}\\neq f_{X^{n}\\setminus X}( x_{i})]\\bigg),$$\n",
    "where $\\mathcal{P}(X^{n})$ is the set of all subsets of $X^{n}$. For all $i=1,2\\dots,n$ denote the label of $m$-th closest neighbor of $x_{i}$ in $X^{n}\\setminus \\{x_{i}\\}$ by $y_{i}^{m}$. Show that \n",
    "$$L_{k}OCV=\\sum_{m=1}^{k}\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq y_{i}^{m}]}_{K_{m}(X^{n})}\\frac{C_{n-1-m}^{n-k-1}}{C_{n-1}^{k-1}},$$\n",
    "where $K_{m}(X^{n})$ is called the compactness profile of $X^{n}$, i.e. the fraction of objects whose $m$-th nearest neighbor has different label. For convenience assume that all the pairwise distances between objects are different.\n",
    "### Your solution:\n",
    "\n",
    "$$L_{k}OCV=\\frac{1}{C_{n}^{k}}\\cdot\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k} \\underbrace{\\bigg(\\sum_{i\\in X}[y_{i}\\neq f_{X^{n}\\setminus X}( x_{i})]\\bigg)}_{N_i}$$\n",
    "\n",
    "\n",
    "And the claim is that:  $N_i = \\sum_{m=1}^k[y_i \\not = y_i^{m}] C_{n-m-1}^{n-k-1} $ because $N_i$ yealds a number of such subsets of $X$ in which $x_i$ is in the test split of a subset and $f_{X^{n}\\setminus X}( x_{i})$ predicts a label for $x_i$ incorrectly. Such observation holds true for subsets whose first $m$ instances from $x_i^{0}, x_i^{1},x_i^{2},...,x_i^{n-1},  $ are in the test split, and $m^{th}$ neighbour is in the traning split and has an opposite labes, thus $[y_i \\not = y_i^{m}]$. Number of such subsets is equal to the number of possibilities chosing $n-k-1$ traning instances left from $n-m-1$ instances of whole set can be expressed as $C_{n-m-1}^{n-k-1} $. And thuse writing the firs sum and the rest a little differently we can get:\n",
    "\n",
    "$$\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k} =  \\sum_{i=1}^n$$\n",
    "\n",
    "$$L_{k}OCV = \\sum_{i=1}^n \\sum_{m=1}^k \\frac{1}{k C_{n}^k} [y_i \\not = y_i^{m}] C_{n-m-1}^{n-k-1}$$\n",
    "\n",
    "$$L_{k}OCV = \\sum_{i=1}^n \\sum_{m=1}^k \\frac{1}{n}\\frac{1}{  C_{k-1}^{n-1}} [y_i \\not = y_i^{m}] C_{n-m-1}^{n-k-1}$$\n",
    "\n",
    "$$L_{k}OCV = \\sum_{m=1}^k  \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{ C_{n-1}^{k-1}} [y_i \\not = y_i^{m}] C_{n-m-1}^{n-k-1}$$\n",
    "\n",
    "$$L_{k}OCV = \\sum_{m=1}^k  \\frac{1}{n} \\sum_{i=1}^n  [y_i \\not = y_i^{m}] \\frac{C_{n-m-1}^{n-k-1}}{ C_{n-1}^{k-1}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Bootstrap (1 point)\n",
    "Let the subsample $\\hat{X}^{n}$ of size $n$ be generated from sample $X^{n}=\\{\\boldsymbol{x}_{1},\\dots\\boldsymbol{x}_{n}\\}$ by bootstrap procedure. Find the probability that object $x_{i}$ is not present in $\\hat{X}^{n}$ and compute the limit of this probability for $n\\rightarrow\\infty$.\n",
    "### Your solution:\n",
    "\n",
    "The probability of an object not beeing in the $\\hat{X}^n$ is expressed as follows $ P = (1-\\frac{1}{n})^n$ then is it's limit is:\n",
    "\n",
    "$$ lim_{n\\rightarrow\\infty}(1-\\frac{1}{n})^n = e^{-1} \\approx 0.367 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6. Decision Tree Leaves (1+1=2 points)\n",
    "\n",
    "Consider a leaf of a binary decision tree which consists of object-label pairs $(x_{1},y_{1}),\\dots,(x_{n},y_{n})$. The prediction $\\hat{y}$ of this leaf is defined to minimize the loss on the training sample.\n",
    "\n",
    "* For a classification tree for K classes ($y_{i}\\in\\{1,2,\\dots,K\\}$) and zero-one loss $L(y,\\hat{y})=[y\\neq \\hat{y}]$, find the optimal prediction in the leaf.\n",
    "\n",
    "* For a regression tree ($y_{i}\\in\\mathbb{R}$) and squared percentage error loss $L(y,\\hat{y})=\\frac{(y-\\hat{y})^{2}}{y^2}$ find the optimal prediction in the leaf.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Your solution:\n",
    "\n",
    "* For a classification tree for K classes ($y_{i}\\in\\{1,2,\\dots,K\\}$) and zero-one loss $L(y,\\hat{y})=[y\\neq \\hat{y}]$, an optimal prediction that minimizes  $\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq\\hat{y}]$ is the majority label(the moset frequent class):\n",
    "$$\\hat{y}=\\underset{k=1,2,\\dots,K}{\\operatorname{argmax}}\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}=k].$$\n",
    "\n",
    "* For a regression tree ($y_{i}\\in\\mathbb{R}$) and squared percentage error loss $L(y,\\hat{y})=\\frac{(y-\\hat{y})^{2}}{y^2}$, an optimal prediction that minimizes $\\frac{1}{n} \\sum_{i=1}^n\\frac{(y_i-\\hat{y})^{2}}{y_i^2}$\n",
    "\n",
    "$$ -2 \\sum_{i =1}^n \\frac{y_i - \\hat{y}}{y_i^2}= 0 $$\n",
    "$$ \\sum_{i=1}^n \\frac{\\hat{y}}{y_i^2} = \\sum_{i=1}^n \\frac{1}{y_i} $$\n",
    "$$ \\hat{y} = \\frac{\\sum_{i=1}^n  \\frac{1}{y_i} }{\\sum_{i=1}^n  \\frac{1}{y_i^2}}$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7. Classification (1 point)\n",
    "Let objects $\\boldsymbol{x}_{1},\\dots,\\boldsymbol{x}_{n}$ have binary labels $y_{1}, y_{2},\\dots,y_{n}\\in\\{0,1\\}$. Let the classifier $f$ assign to objects probabilities of being from class $1$. Without loss of generality assume that $f(\\boldsymbol{x_{i}})<f(\\boldsymbol{x_{j}})$ for all $i<j$. Denote the number of objects of $X^{n}$ from class $1$ by $n_{1}=\\sum_{i=1}^{n}[y_{i}=1]$. Show that \n",
    "$$S_{\\text{ROC}}(f,X^{n})=\\frac{1}{n_{1}(n-n_{1})}\\sum_{i<j}[y_{i}<y_{j}]$$\n",
    "where $S_{\\text{ROC}}(f,X^{n})$ is the Area Under ROC of classifier $f$.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8. Kernels (1+1=2 points)\n",
    "Kernel $K(x,y)$ corresponds to dot product of feature maps $\\varphi$ and therefore $K(x,y) = \\langle \\varphi(x), \\varphi(y) \\rangle$. Derive functions $\\varphi$ for the following kernels:\n",
    "* $K(x,y)=\\langle x, y \\rangle ^ d$;\n",
    "* $K(x,y)= \\left(c + \\langle x, y \\rangle \\right)^ d$  with $c\\geq 0$;\n",
    "\n",
    "### Your solution:\n",
    "* $K(x,y)=\\langle x, y \\rangle ^d = (\\sum_{i=1}^N x_iy_i)^d = (\\sum_{i_1=1}^N x_{i_1} y_{i_1})...(\\sum_{i_p=1}^N x_{i_p} y_{i_p})$\n",
    "\n",
    "    From the rule: $ (\\sum a_i)(\\sum b_j) = \\sum\\sum a_i b_j  $\n",
    "    \n",
    "    $(\\sum_{i_1=1}^N x_{i_1} y_{i_1})...(\\sum_{i_p=1}^N x_{i_p} y_{i_p}) = \\sum_{i_1=1}^N  ... \\sum_{i_p=1}^N \\underbrace{ (x_{i_{p}} ... x_{i_{p}}) }_{\\varphi_{i}{(x)}} \\underbrace{(y_{i_{1}} ... y_{i_{p}})}_{\\varphi_{i}{(y)}} = \\langle \\varphi{(x)},\\varphi{(y)}\\rangle \\space $,     so that is $\\varphi{(x)}$ and $\\varphi{(y)}$\n",
    "    \n",
    "    $ r = (r_1,...,r_N)$\n",
    "    \n",
    "    $r_1+r_2+...+r_N = p, r_i \\in [0,p] $\n",
    "    \n",
    "    $\\alpha_{{r_1, ... ,r_N}} = \\sqrt{\\frac{p!}{r_1!,...,r_N!}} $\n",
    "    \n",
    "    $\\varphi_{r}{(x)} = \\alpha_{{r_1, ... ,r_N}} x_1^{r_1}...x_N^{r_N}$\n",
    "    \n",
    "    $\\varphi_{r}{(y)} = \\alpha_{{r_1, ... ,r_N}} y_1^{r_1}...y_N^{r_N}$\n",
    "    \n",
    "    Thus the kernal can be expressed as:\n",
    "\n",
    "    $K(x,y) = \\sum_{r = (r_1,...,r_N)} \\alpha_{{r_1, ... ,r_N}} x_1^{r_1}...x_N^{r_N}  y_1^{r_1}...y_N^{r_N} = \\sum_{r}\\varphi_{r}{(x)}\\varphi_{r}{(y)}$\n",
    "    \n",
    "    \n",
    "* The second part of the problem can be reduced and presented as the first one with a simple manipulation:\n",
    "\n",
    "    $c + \\langle x , y \\rangle  = c + \\sum_{i=1}^N x_i y_i =\\underbrace{[\\sqrt{c},x_1,x_2,...,x_n]}_{X'}\\underbrace{[\\sqrt{c},y_1,y_2,...,y_n]^T}_{Y'}$\n",
    "    \n",
    "    Thus the problem reduces to:\n",
    "    \n",
    "    $K(X',Y')=\\langle X' , Y' \\rangle ^d $\n",
    "    \n",
    "    And thus can be solved as in the previous case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9. Kernel Methods (1 point)\n",
    "Prove that Gaussian Kernel $K(x,x')=\\exp(-\\|x-x'\\|^{2})$ is positive definite. You do not need to prove that the linear kernel is positive definite.\n",
    "### Your solution:\n",
    "\n",
    "As the following expression can be decomposed as such:\n",
    "$$ \\|x-x'\\|^{2} = \\|x\\|^2 -  2 x^\\top x' +  \\|x'\\|^2 $$\n",
    "Then the Gaussian Kernel can be written as follows:\n",
    "$$K(x,x')=\\exp(-\\|x-x'\\|^{2})=\\exp{( -\\|x\\|^2 +2 x^\\top x' -  \\|x'\\|^2 )} = \\exp(-\\|x\\|^2)\\exp(-\\|x'\\|^2)\\exp(2x^\\top x')$$\n",
    "\n",
    "Here we have a little trick we can perform on the above expression. Prove $\\sum_{i,j=1}^n a_i,a_j\\exp(-(x_i - x_j)^2) \\geq 0 $ for $\\forall x \\in X$ and $a \\in \\mathbb{R}$\n",
    "\n",
    "$$ \\sum_{i,j=1}^n a_i,a_j\\exp(-(x_i - x_j)^2) = \\sum_{i,j=1}^n a_i,a_j\\exp(-x_i^2 - x_j^2 + 2x_ix_j) = \\sum_{i,j=1}^n \\underbrace{\\frac{a_i}{e^{x_i^2}}}_{a_i'}  \\underbrace{\\frac{a_j}{e^{x_j^2}}}_{a_j'}  \\exp(2x_ix_j)  =  \\sum_{i,j=1}^n a_i'  a_j' \\exp(2x_ix_j) $$\n",
    "\n",
    "\n",
    "As properties of **Positive Definite** kernals state: \n",
    "\n",
    "$\\exp(K(x,x')) = lim_{n \\to \\infty} \\sum_{n=0}^N \\frac{K^n(x,x')}{n!}$ is a **Positive Definite** if K is a PD kernal\n",
    "\n",
    "\n",
    "$$\\exp(2x^\\top x') = 1 + 2 x^\\top x' + \\frac{2^2}{2!} (x^\\top x')^2 + \\dots = \\sum\\limits_{i=0}^{\\infty} \\frac{(2x^\\top x')^i}{i!} $$\n",
    "\n",
    "\n",
    "Thus we can say that  $\\sum_{i,j=1}^n a_i,a_j\\exp(-(x_i - x_j)^2) \\geq 0 $ for  $K(x,x') = \\exp(2x^\\top x')$ is a **Positive Definite** kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10. Support Vector Machine (1 point)\n",
    "Show that for two-class SVM classifier the following upper bound on accuracy leave-one-out-cross-validation estimate holds true:\n",
    "$$L_{1}OCV=\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\not = f_{i}(x_{i})]\\leq \\frac{|SV|}{n},$$\n",
    "where for all $i=1,\\dots,n$ $f_{i}(x_{i})$ is SVM fitted on the entire data without observation $(x_{i},y_{i})$ and $|SV|$ is the number of support vectors of SVM fit on the entire data.\n",
    "### Your solution:\n",
    "Could not come up with a consice math notation for this solution but the idea benind is very simple. For the model fitted and on the whole data sent we will have $K$ support vectors and a hyperplane. As long as for all $f_{i}(x_{i})$ the observation $(x_{i},y_{i}) \\notin K, [y_{i}=f_{i}(x_{i})] - $ is true, thus no error. If $(x_{i},y_{i}) \\in K, [y_{i}=f_{i}(x_{i})]$ - might not be true, and in the worst case, $ number  of errors = |SV| $, which still holds $ L_{1}OCV= \\sum_{i=1}^{n}[y_{i}=f_{i}(x_{i})]\\leq |SV|$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
